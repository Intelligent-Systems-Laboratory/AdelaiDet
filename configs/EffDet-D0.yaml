MODEL:
  META_ARCHITECTURE: "RetinaNet_SiLu"
  BACKBONE:
    NAME: "build_effnet_bifpn_backbone"
  EffNet:
    STEM_W: 32
    STRIDES: [1, 2, 2, 2, 1, 2, 1]
    DEPTHS: [1, 2, 2, 3, 3, 4, 1]
    WIDTHS: [16, 24, 40, 80, 112, 192, 320]
    EXP_RATIOS: [1, 6, 6, 6, 6, 6, 6]
    KERNELS: [3, 3, 5, 3, 5, 5, 3]
    HEAD_W: 1280
    MODEL_ACTIVATION_FUN: "silu"   # EffDet uses silu
    MODEL_ACTIVATION_INPLACE: False
    OUT_FEATURES: ["s3", "s4", "s5", "s6", "s7"]
    OUT_FEATURE_CHANNELS: [40, 80, 112, 192, 320]
    OUT_FEATURE_STRIDES: [4, 8, 16, 16, 32]

    BN_EPS: 1e-5
    BN_MOM: 0.9
  BiFPN:
    IN_FEATURES: ["s3", "s4", "s5", "s6", "s7"]
    OUT_CHANNELS: 64
    NUM_REPEATS: 3
    NORM: "BN"
  ANCHOR_GENERATOR:
    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map
    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)
  RETINANET:
    # Based on Base-RetinaNet.yaml
    IOU_THRESHOLDS: [0.4, 0.5]
    IOU_LABELS: [0, -1, 1]
    SMOOTH_L1_LOSS_BETA: 0.0

    # Based on EffDet paper
    FOCAL_LOSS_GAMMA: 2.0
    FOCAL_LOSS_ALPHA: 0.25
  
    NUM_CONVS: 3

    NORM: "BN"
    ACTIVATION_FUN: "silu"
  # RPN:
  #   IN_FEATURES: ["p3", "p4", "p5", "p6", "p7"]
  #   PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level
  #   PRE_NMS_TOPK_TEST: 1000  # Per FPN level
  #   # Detectron1 uses 2000 proposals per-batch,
  #   # (See "modeling/rpn/rpn_outputs.py" for details of this legacy issue)
  #   # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.
  #   POST_NMS_TOPK_TRAIN: 1000
  #   POST_NMS_TOPK_TEST: 1000
  # ROI_HEADS:
  #   NAME: "StandardROIHeads"
  #   IN_FEATURES: ["p3", "p4", "p5", "p6", "p7"]
DATASETS:
  TRAIN: ("coco_2017_train",)
  TEST: ("coco_2017_val",)
SOLVER:
  IMS_PER_BATCH: 16
  # BASE_LR: 0.02
  STEPS: (60000, 80000)
  MAX_ITER: 90000

  # Based on EffDet Paper Experiments
  MOMENTUM: 0.9
  WEIGHT_DECAY: 0.00004
  WEIGHT_DECAY_NORM: 0.00004 # not sure wheter to use 4e-5 on both


  # from EffDet Paper: "Learning rate is linearly increased from 
  # 0 to 0.16 in the first training epoch and 
  # then annealed down using cosine decay rule" 
  BASE_LR: 0.001  # default
  GAMMA: 0.1  # default
  # The iteration number to decrease learning rate by GAMMA.
  STEPS: (30000,) 

  # WARMUP_FACTOR: 1.0 / 1000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 1600
  WARMUP_METHOD: "linear"


INPUT:
  # MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)
  MIN_SIZE_TRAIN: (512, )
VERSION: 2
